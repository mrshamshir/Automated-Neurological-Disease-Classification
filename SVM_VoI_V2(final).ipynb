{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "-IuO6t0a9l_X",
        "KwnJyu-6EYVD",
        "qeFB8IFfDyET",
        "tTtpEKN4GNtY",
        "rlQWwupSGtL-",
        "PiIXgA9AHBUb",
        "-Z4tJGaUHX9i",
        "zbElRYAyLggo",
        "KxaLf8BgLpqU"
      ],
      "mount_file_id": "1HcciULg5PaFTn5vUJEFkZItO3aU1lK4q",
      "authorship_tag": "ABX9TyNzZaEfEJyNFzNQbjYoWNIP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrshamshir/Automated-Neurological-Disease-Classification/blob/main/SVM_VoI_V2(final).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports and data loading"
      ],
      "metadata": {
        "id": "-IuO6t0a9l_X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rITzog3Y8V8_"
      },
      "outputs": [],
      "source": [
        "# import necessary libraries for entire notebook\n",
        "\n",
        "from pathlib import Path\n",
        "import nibabel as nib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# path to train datasets, labels and VoI template files\n",
        "\n",
        "train_rCBF = Path(\"/content/drive/MyDrive/Assignment/training_images_rcbf.nii\")\n",
        "train_DAT = Path(\"/content/drive/MyDrive/Assignment/training_images_sbr.nii\")\n",
        "\n",
        "voi_template = Path(\"/content/drive/MyDrive/Assignment/voi_template.nii\")\n",
        "\n",
        "labels = pd.read_csv(\"/content/drive/MyDrive/Assignment/Diagnoses_of_training_data.csv\")"
      ],
      "metadata": {
        "id": "LDhpVPhJ9orJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load NIfTI and extract image data\n",
        "train_PET_rCBF = nib.load(train_rCBF)\n",
        "train_data_rCBF = train_PET_rCBF.get_fdata()\n",
        "\n",
        "train_PET_DAT = nib.load(train_DAT)\n",
        "train_data_DAT = train_PET_DAT.get_fdata()\n",
        "\n",
        "voi = nib.load(voi_template)\n",
        "data_voi = voi.get_fdata()"
      ],
      "metadata": {
        "id": "JsMnygyL96DK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature engineering and dataset creation"
      ],
      "metadata": {
        "id": "IHVvSTyN-FlT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_xdata(rCBF, DAT):\n",
        "    # combine two images of same subjects\n",
        "    res = np.stack((rCBF, DAT), axis = 3)\n",
        "    res = np.transpose(res, (4, 0, 1, 2, 3))\n",
        "    return res\n",
        "\n",
        "xdata_train = create_xdata(train_data_rCBF, train_data_DAT)\n",
        "print(xdata_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkiP9yPb-M55",
        "outputId": "b2623b2c-bf0b-4080-d31e-bb63da31ca3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(40, 64, 64, 64, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_regional_extraction(data, template):\n",
        "    \"\"\"\n",
        "        Extracts and calculates the mean values from specified regions within a multi-dimensional dataset\n",
        "        (e.g. brain), based on a given template.\n",
        "\n",
        "        Parameters:\n",
        "        - data (numpy.ndarray): The input dataset from which to extract mean values. Expected to have dimensions\n",
        "        where the first dimension is the number of observations (e.g., subjects, time points) and the\n",
        "        subsequent dimensions correspond to spatial dimensions (e.g., in neuroimaging, x, y, z coordinates of brain voxels).\n",
        "        - template (numpy.ndarray): A template or mask with the same spatial dimensions as `data`, where each unique\n",
        "        non-zero value represents a different region of interest (ROI). The regions are defined by distinct integer\n",
        "        values, with typically 0 representing background or non-interest areas.\n",
        "\n",
        "        Returns:\n",
        "        - numpy.ndarray: A 2D array where each row corresponds to an observation from the input `data` and each column\n",
        "        represents the mean value of a region defined in `template`. If `data` includes dimensions beyond the spatial\n",
        "        (e.g., channels, time points), these are aggregated into the columns as well, resulting in a flat array of\n",
        "        mean regional values for each observation.\n",
        "\n",
        "        This method is useful for summarizing spatially distributed data according to predefined regions, such as\n",
        "        extracting mean regional brain activity from neuroimaging data based on anatomical or functional regions of interest.\n",
        "    \"\"\"\n",
        "\n",
        "    # Identify unique regions and their occurrence in the template\n",
        "    tp_values, tp_counts = np.unique(template, return_counts=True)\n",
        "\n",
        "    mean_region_values = []\n",
        "\n",
        "    # Iterate through each region value and its count\n",
        "    for value, count in zip(tp_values, tp_counts):\n",
        "        # Process regions with value greater than zero, all non-backgrounds\n",
        "        if value > 0:\n",
        "            # Find the indices in the template corresponding to the current region\n",
        "            indices = np.where(template == value)\n",
        "            # Extract values from `data` at these indices, across all observations\n",
        "            region_values = data[:, indices[0], indices[1], indices[2], :]\n",
        "            # Compute the mean of these values, collapsing the spatial dimensions\n",
        "            mean_region_value = np.mean(region_values, axis=1)\n",
        "            mean_region_values.append(mean_region_value)\n",
        "\n",
        "    # Convert list of mean values into a numpy array and reshape for output\n",
        "    return np.array(mean_region_values).transpose((1,0,2)).reshape((data.shape[0], -1))\n",
        "\n",
        "\n",
        "train_mr_features = mean_regional_extraction(xdata_train, data_voi, )\n",
        "print(train_mr_features.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgYNE37m-Byv",
        "outputId": "7b398940-a768-4462-fbac-356fd863452b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(40, 106)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into training and validation sets with 80% for training and 20% for validation, preserving class distribution.\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(train_mr_features,\n",
        "                                                    np.array(labels['diagnose']),\n",
        "                                                    test_size=0.2, random_state=13,\n",
        "                                                    stratify=labels['diagnose'])"
      ],
      "metadata": {
        "id": "zTbuDWyyDJeK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_val.shape)\n",
        "print(y_val.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2vCw-5oDmWP",
        "outputId": "8435cd21-eafe-4bfe-e2fd-e559fae8cbd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32, 106)\n",
            "(32,)\n",
            "(8, 106)\n",
            "(8,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('train dataset', np.unique(y_train, return_counts=True))\n",
        "print('validation dataset', np.unique(y_val, return_counts=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iO3YVkLuDoRd",
        "outputId": "318be621-042f-4ab0-f59a-9ff20e9a68e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train dataset (array([1, 2, 3, 4]), array([8, 8, 8, 8]))\n",
            "validation dataset (array([1, 2, 3, 4]), array([2, 2, 2, 2]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### normalization for each feature"
      ],
      "metadata": {
        "id": "KwnJyu-6EYVD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_mean_std(data):\n",
        "    \"\"\"\n",
        "        Calculate the mean and standard deviation for each feature across the dataset.\n",
        "\n",
        "        Args:\n",
        "        - data (numpy.ndarray): Input dataset with shape (num_samples, num_features).\n",
        "\n",
        "        Returns:\n",
        "        - mean (numpy.ndarray): Mean value for each feature, with shape (num_features,).\n",
        "        - std (numpy.ndarray): Standard deviation for each feature, with shape (num_features,).\n",
        "    \"\"\"\n",
        "    # Reshape the dataset to collapse the sample dimension\n",
        "    flattened_data = data.reshape(-1, data.shape[-1])\n",
        "\n",
        "    # Calculate mean and standard deviation for each feature\n",
        "    mean = np.mean(flattened_data, axis=0)\n",
        "    std = np.std(flattened_data, axis=0)\n",
        "\n",
        "    return mean, std\n",
        "\n",
        "\n",
        "\n",
        "def normalizer(data, mean, std):\n",
        "    \"\"\"\n",
        "        Normalize the input data using the provided mean and standard deviation.\n",
        "\n",
        "        Args:\n",
        "        - data (numpy.ndarray): Input data to be normalized.\n",
        "        - mean (numpy.ndarray): Mean values for each feature, with shape (num_features,).\n",
        "        - std (numpy.ndarray): Standard deviation for each feature, with shape (num_features,).\n",
        "\n",
        "        Returns:\n",
        "        - ret (numpy.ndarray): Normalized data.\n",
        "    \"\"\"\n",
        "    ret = (data - mean) / std\n",
        "    return ret\n",
        "\n"
      ],
      "metadata": {
        "id": "ZGnsf6JMEYAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We should calculate mean and std only based on the data that we are going to train our model on.\n",
        "\n",
        "mean, std = calculate_mean_std(X_train)\n",
        "print(\"Mean for each channel:\", mean.shape)\n",
        "print(\"Standard deviation for each channel:\", std.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqoElpzwFyHf",
        "outputId": "1b32c4ac-59c8-45e3-afbf-0318fe12df7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean for each channel: (106,)\n",
            "Standard deviation for each channel: (106,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nX_train = normalizer(X_train, mean, std)\n",
        "nX_val = normalizer(X_val, mean, std)\n",
        "print(nX_train.shape)\n",
        "print(nX_val.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "st76DbaDGD21",
        "outputId": "ee4c4e3c-8824-494b-ccce-f43d42a42371"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32, 106)\n",
            "(8, 106)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train and evaluate SVM model"
      ],
      "metadata": {
        "id": "qeFB8IFfDyET"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_svm(gt, preds):\n",
        "    \"\"\"\n",
        "        Evaluate SVM model performance using ground truth and predicted labels.\n",
        "\n",
        "        Parameters:\n",
        "        - gt (array-like): Ground truth labels.\n",
        "        - preds (array-like): Predicted labels.\n",
        "\n",
        "        Prints:\n",
        "        - Validation accuracy.\n",
        "        - Confusion matrix.\n",
        "        - Precision and recall scores for each class.\n",
        "    \"\"\"\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(gt, preds)\n",
        "    print(\"Validation Accuracy:\", accuracy)\n",
        "\n",
        "    # Calculate confusion matrix\n",
        "    conf_matrix = confusion_matrix(gt, preds)\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(conf_matrix)\n",
        "\n",
        "    # Calculate precision and recall\n",
        "    precision = precision_score(gt, preds, average=None)\n",
        "    recall = recall_score(gt, preds, average=None)\n",
        "\n",
        "    print(\"Precision:\", precision)\n",
        "    print(\"Recall:\", recall)\n"
      ],
      "metadata": {
        "id": "u0T4uzLUDxrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### train and test first SVM on train and validation dataset without feature normalization."
      ],
      "metadata": {
        "id": "tTtpEKN4GNtY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train an SVM classifier\n",
        "svm_classifier1 = svm.SVC(kernel='linear')\n",
        "svm_classifier1.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "KEiTPz69EGLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict using the SVM classifier and evaluate\n",
        "predicted_labels = svm_classifier1.predict(X_val)\n",
        "evaluate_svm(y_val, predicted_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gma39gOTGisQ",
        "outputId": "f0416f46-a2b7-4d32-ebf4-4f73e6fdd6a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.75\n",
            "Confusion Matrix:\n",
            "[[2 0 0 0]\n",
            " [0 1 1 0]\n",
            " [0 0 2 0]\n",
            " [0 1 0 1]]\n",
            "Precision: [1.         0.5        0.66666667 1.        ]\n",
            "Recall: [1.  0.5 1.  0.5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### train and test second SVM on train and validation dataset with feature normalization."
      ],
      "metadata": {
        "id": "rlQWwupSGtL-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train an SVM classifier\n",
        "svm_classifier2 = svm.SVC(kernel='linear')\n",
        "svm_classifier2.fit(nX_train, y_train)"
      ],
      "metadata": {
        "id": "vFnPomHZGxUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict using the SVM classifier\n",
        "predicted_labels = svm_classifier2.predict(nX_val)\n",
        "\n",
        "evaluate_svm(y_val, predicted_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GF3qEUT7G3in",
        "outputId": "2d2cce02-ebed-4443-ac04-7e5d534902a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 1.0\n",
            "Confusion Matrix:\n",
            "[[2 0 0 0]\n",
            " [0 2 0 0]\n",
            " [0 0 2 0]\n",
            " [0 0 0 2]]\n",
            "Precision: [1. 1. 1. 1.]\n",
            "Recall: [1. 1. 1. 1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final model\n",
        "Train our final model on all of training data.\n",
        "Predict labels for test data that we don't have their labels and save them on CSV file."
      ],
      "metadata": {
        "id": "PiIXgA9AHBUb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the test data"
      ],
      "metadata": {
        "id": "-Z4tJGaUHX9i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# path to train datasets, labels and VoI template files\n",
        "\n",
        "test_rCBF = Path(\"/content/drive/MyDrive/Assignment/test_images_rcbf.nii\")\n",
        "test_DAT = Path(\"/content/drive/MyDrive/Assignment/test_images_sbr.nii\")"
      ],
      "metadata": {
        "id": "1KBwyj_8HXyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load NIfTI and extract image data\n",
        "\n",
        "test_PET_rCBF = nib.load(test_rCBF)\n",
        "test_data_rCBF = test_PET_rCBF.get_fdata()\n",
        "\n",
        "test_PET_DAT = nib.load(test_DAT)\n",
        "test_data_DAT = test_PET_DAT.get_fdata()"
      ],
      "metadata": {
        "id": "ntWoS4h8HdhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset creation, feature engineering and feature normalization"
      ],
      "metadata": {
        "id": "zbElRYAyLggo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xdata_test = create_xdata(test_data_rCBF, test_data_DAT)\n",
        "print(xdata_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oX9bsgCtH53p",
        "outputId": "8a2e88a5-4458-40fa-f1e7-bb6ba17045a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(41, 64, 64, 64, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_mr_features = mean_regional_extraction(xdata_train, data_voi)\n",
        "print(train_mr_features.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3cATijNlKxhG",
        "outputId": "7a3ccd91-f443-4a8b-8ee8-cefe318554b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(40, 106)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_mr_features = mean_regional_extraction(xdata_test, data_voi)\n",
        "print(test_mr_features.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c35ISrzvKzaC",
        "outputId": "56017b89-e986-4875-9986-6e72a0a9f104"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(41, 106)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This time we are going to train our model on all of training data,\n",
        "# so we calcualte mean and std for all of them.\n",
        "\n",
        "mean, std = calculate_mean_std(train_mr_features)\n",
        "print(\"Mean for each channel:\", mean.shape)\n",
        "print(\"Standard deviation for each channel:\", std.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B00TNDY0IAiU",
        "outputId": "b3328f5b-7f93-4832-8e59-fc13a406a7ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean for each channel: (106,)\n",
            "Standard deviation for each channel: (106,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_normal = normalizer(train_mr_features, mean, std)\n",
        "print(train_normal.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXJBvX6gKsmd",
        "outputId": "63d696db-e629-4ab8-b3ce-8ecddf3d9153"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(40, 106)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_normal = normalizer(test_mr_features, mean, std)\n",
        "print(test_normal.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8PSVPFE9LL0J",
        "outputId": "f86ac4b8-77b6-4c27-9469-d202c770a456"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(41, 106)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train SVM model, predict on test and save to file"
      ],
      "metadata": {
        "id": "KxaLf8BgLpqU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "svm_classifier3 = svm.SVC(kernel='linear')\n",
        "svm_classifier3.fit(train_normal, np.array(labels['diagnose']))"
      ],
      "metadata": {
        "id": "K52I_tdZLNUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_preds3 = svm_classifier3.predict(test_normal)\n",
        "print(test_preds3.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03RQpzYUL4ZK",
        "outputId": "f6b0f08d-3d22-43be-b912-c7b040d534e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(41,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the DataFrame to a CSV file, including patient number\n",
        "\n",
        "index_array = np.arange(1, test_preds3.shape[0] + 1)\n",
        "combined_array = np.column_stack((index_array, test_preds3))\n",
        "\n",
        "df = pd.DataFrame(combined_array, columns=['patient_number', 'SVM_predication'])\n",
        "df.to_csv(\"patient_predictions.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "EPKsZPyYMBrg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}