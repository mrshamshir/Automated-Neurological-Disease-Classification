# This readme will be updated in the future

# Automated-Neurological-Disease-Classification

To automatically classify 3D PET images into four diseases, we explored four closely related approaches, each offering distinct predictions, as detailed in the attached Excel file. No additional preprocessing was required given the MNI space normalization of the provided images. However, each approach employed different methods of normalization for optimal performance.\n
Our initial approach involved feature extraction using the provided VOI template. By calculating the mean value for each brain structure in both rCBF and DAT images, we derived 106 features per subject. Given the limited training data (40 subjects), we opted for SVM classification due to its effectiveness with small datasets. Notably, feature normalization enhanced model performance, making this approach our most promising.
In the pursuit of feature extraction, we turned to neural networks, particularly CNNs, known for their direct utilization of images. Despite the data scarcity, we enhanced model robustness through normalization and data augmentation techniques like random rotation, translation, and scale. ResNet is one of the best feature extraction and classification models, but it is designed for 2D tasks. We designed our own 3D ResNet model and trained it from scratch. This model can do feature extraction and classification on its own and will combine features from rCBF and DAT data. We used the idea of late fusion and designed another ResNet that processes rCBF and DAT images separately with ResNet branches and, in the end, will combine features. We also used cross-validation to find the best optimizer for these models. Predictions of both models on test subjects are available in an Excel file. To enhance the performance of these models, we can first train them on a similar but very large dataset, like brain MR images and then fine-tune the model on our own dataset.
In CNN-based models, it is crucial to interpret the modelâ€™s decisions. For this purpose, we explored methods under the domain of Explainable AI. We chose Grad-CAM, which utilizes the gradients of the classification score with respect to the final convolutional feature map, to identify the parts of an input image that most impact the classification score. The original method works on 2D images, so we modified it for our own data. This aspect of our research is particularly significant as it enhances the transparency and trustworthiness of our models. For more details, please refer to the long report.
Given the constraint of a small amount of training data, we strategically employed SVM for classification. We used features that have been extracted with the ResNet model to train an SVM classifier. In other words, we extracted features with CNNs and used both MLP and SVM for classification. The predictions of this model are also available in the attached Excel file.
In summary, our diverse approaches sought to maximize classification accuracy while considering model interpretability and generalization, particularly in the context of limited training data.
