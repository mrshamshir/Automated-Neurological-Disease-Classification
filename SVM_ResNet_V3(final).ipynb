{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "pfg-AXwW4_T6",
        "wcJU5pz25C-I",
        "R6OHPeieuSXj",
        "wnftW2fYQ9qd",
        "C7fxeyOWiv_7",
        "o1Ppcgn6uIJ5",
        "7IXw7A9ZkQBX",
        "NjnbRXLasGrM",
        "XwVjzmQtuyQS",
        "9DZ787rL4orl",
        "XhVmBC_24n5k",
        "PiIXgA9AHBUb",
        "-Z4tJGaUHX9i",
        "zbElRYAyLggo",
        "KxaLf8BgLpqU"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrshamshir/Automated-Neurological-Disease-Classification/blob/main/SVM_ResNet_V3(final).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfg-AXwW4_T6"
      },
      "source": [
        "### Imports and data loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "biOdJirl0jiW"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import nibabel as nib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import pickle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, accuracy_score\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch import Tensor\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sklearn import svm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6Zdd33i0lqP"
      },
      "outputs": [],
      "source": [
        "# path to train datasets and labels\n",
        "\n",
        "train_rCBF = Path(\"/content/drive/MyDrive/Assignment/training_images_rcbf.nii\")\n",
        "train_DAT = Path(\"/content/drive/MyDrive/Assignment/training_images_sbr.nii\")\n",
        "\n",
        "labels = pd.read_csv(\"/content/drive/MyDrive/Assignment/Diagnoses_of_training_data.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TvBLXMcT1Rc1"
      },
      "outputs": [],
      "source": [
        "# Load NIfTI and extract image data\n",
        "train_PET_rCBF = nib.load(train_rCBF)\n",
        "train_data_rCBF = train_PET_rCBF.get_fdata()\n",
        "\n",
        "train_PET_DAT = nib.load(train_DAT)\n",
        "train_data_DAT = train_PET_DAT.get_fdata()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcJU5pz25C-I"
      },
      "source": [
        "### Dataset Creation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Loading data"
      ],
      "metadata": {
        "id": "R6OHPeieuSXj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFZEQ_vGotLx",
        "outputId": "e37af37a-c397-4961-f77c-05d7a75cc453"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(40, 2, 64, 64, 64)\n"
          ]
        }
      ],
      "source": [
        "def create_xdata(rCBF, DAT):\n",
        "    # combine two images of same subjects\n",
        "    res = np.stack((rCBF, DAT), axis = 3)\n",
        "    res = np.transpose(res, (4, 3, 0, 1, 2))\n",
        "    return res\n",
        "\n",
        "xdata = create_xdata(train_data_rCBF, train_data_DAT)\n",
        "print(xdata.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Define Normalization and dataset classes"
      ],
      "metadata": {
        "id": "wnftW2fYQ9qd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cOgQX2w_thYG"
      },
      "outputs": [],
      "source": [
        "def calculate_mean_std(data):\n",
        "    \"\"\"\n",
        "        Calculate the mean and standard deviation for each channel across all samples in the input data.\n",
        "\n",
        "        Args:\n",
        "        - data (numpy.ndarray): Input data with shape (num_samples, num_channels, depth, height, width).\n",
        "\n",
        "        Returns:\n",
        "        - mean (numpy.ndarray): Mean values for each channel across all samples, with shape (num_channels,).\n",
        "        - std (numpy.ndarray): Standard deviation for each channel across all samples, with shape (num_channels,).\n",
        "    \"\"\"\n",
        "    mean = np.mean(data, axis=(0, 2, 3, 4))\n",
        "    std = np.std(data, axis=(0, 2, 3, 4))\n",
        "    return mean, std\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom transformation to convert numpy array to tensor\n",
        "class ToTensor(object):\n",
        "    def __call__(self, sample):\n",
        "        return torch.from_numpy(sample).float()  # Convert to float tensor\n",
        "\n",
        "\n",
        "class Normalize3D(torch.nn.Module):\n",
        "    def __init__(self, mean, std, inplace=False):\n",
        "        \"\"\"\n",
        "            Initializes the 3D normalization module.\n",
        "\n",
        "            Args:\n",
        "            - mean (array-like): Mean values for each channel.\n",
        "            - std (array-like): Standard deviation values for each channel.\n",
        "            - inplace (bool): If True, normalize the tensor in-place. Default is False.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.mean = torch.tensor(mean, dtype=torch.float32).view(-1, 1, 1, 1)\n",
        "        self.std = torch.tensor(std, dtype=torch.float32).view(-1, 1, 1, 1)\n",
        "        self.inplace = inplace\n",
        "\n",
        "    def forward(self, tensor: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "            Forward pass of the normalization module.\n",
        "\n",
        "            Args:\n",
        "            - tensor (Tensor): Input tensor to be normalized.\n",
        "\n",
        "            Returns:\n",
        "            - Tensor: Normalized output tensor.\n",
        "        \"\"\"\n",
        "        if self.inplace:\n",
        "            tensor.sub_(self.mean).div_(self.std)\n",
        "            return tensor\n",
        "        else:\n",
        "            return (tensor - self.mean) / self.std\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        \"\"\"\n",
        "            Returns a string representation of the normalization module.\n",
        "\n",
        "            Returns:\n",
        "            - str: String representation of the module.\n",
        "        \"\"\"\n",
        "        return f\"{self.__class__.__name__}(mean={self.mean}, std={self.std})\"\n"
      ],
      "metadata": {
        "id": "BOSTmgwothYH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CD9Eng25thYI"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data, labels, transform=None):\n",
        "        \"\"\"\n",
        "            Custom dataset class for handling input data and labels.\n",
        "\n",
        "            Args:\n",
        "            - data (array-like): Input data array.\n",
        "            - labels (array-like): Labels array corresponding to the input data.\n",
        "            - transform (callable, optional): Optional transform to be applied to the input data.\n",
        "\n",
        "        \"\"\"\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "            Returns the length of the dataset.\n",
        "\n",
        "            Returns:\n",
        "            - int: Length of the dataset.\n",
        "        \"\"\"\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "            Retrieves an item from the dataset based on the provided index.\n",
        "\n",
        "            Args:\n",
        "            - idx (int): Index of the item to retrieve.\n",
        "\n",
        "            Returns:\n",
        "            - dict: A dictionary containing the input data and its corresponding label.\n",
        "        \"\"\"\n",
        "        sample = {'input': self.data[idx], 'label': self.labels[idx] - 1}\n",
        "        if self.transform:\n",
        "            sample['input'] = self.transform(sample['input'])\n",
        "        return sample\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "mqNSiU69ilWg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model creation and loading"
      ],
      "metadata": {
        "id": "C7fxeyOWiv_7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicBlock3D(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(BasicBlock3D, self).__init__()\n",
        "        self.conv1 = nn.Conv3d(inplanes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm3d(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv3d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm3d(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class ResNet3D(nn.Module):\n",
        "    def __init__(self, block, layers, num_classes=4, zero_init_residual=False):\n",
        "        super(ResNet3D, self).__init__()\n",
        "        self.inplanes = 64\n",
        "        self.conv1 = nn.Conv3d(2, 64, kernel_size=(7, 7, 7), stride=(2, 2, 2), padding=(3, 3, 3), bias=False)  # Change input channels to 64\n",
        "        self.bn1 = nn.BatchNorm3d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool3d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "        self.avgpool = nn.AdaptiveAvgPool3d((1, 1, 1))\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv3d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm3d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        if zero_init_residual:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, BasicBlock3D):\n",
        "                    nn.init.constant_(m.bn2.weight, 0)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv3d(self.inplanes, planes * block.expansion, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm3d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def feature_extraction(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "62RY8i0yidVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading ResNet3D model"
      ],
      "metadata": {
        "id": "aTaPoX_l9q9S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model = ResNet3D(BasicBlock3D, [2, 2, 2, 2], num_classes=4)\n",
        "path = '/content/drive/MyDrive/output/ML_Task/resnet3d_ML_Task_V4_best_0.0002_SGD_best_on_min_val_loss.pt'\n",
        "\n",
        "checkpoint = torch.load(path, map_location=torch.device('cpu'))\n",
        "loaded_model.load_state_dict(checkpoint)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aio9YaZ-i2WE",
        "outputId": "d38f99fc-5b30-4e3f-be92-de4e5769e6f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## K-Fold experiment"
      ],
      "metadata": {
        "id": "o1Ppcgn6uIJ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Create K-Folds"
      ],
      "metadata": {
        "id": "7IXw7A9ZkQBX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "skf = StratifiedKFold(n_splits=5, random_state=13, shuffle=True)"
      ],
      "metadata": {
        "id": "4VQ224O0i46k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res=skf.split(xdata, np.array(labels['diagnose']))"
      ],
      "metadata": {
        "id": "p4kKNgZOje-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "folds_info=[]\n",
        "\n",
        "for i, (train_index, test_index) in enumerate(skf.split(xdata, np.array(labels['diagnose']))):\n",
        "  print(f\"Fold {i}:\")\n",
        "  print(f\"  Train: index={train_index}\")\n",
        "  print(f\"  Test:  index={test_index}\")\n",
        "  train_fold_x=[]\n",
        "  train_fold_y=[]\n",
        "  val_fold_x=[]\n",
        "  val_fold_y=[]\n",
        "  for idx in train_index:\n",
        "    train_fold_x.append(xdata[idx])\n",
        "    train_fold_y.append(labels['diagnose'][idx])\n",
        "\n",
        "  for idx in test_index:\n",
        "    val_fold_x.append(xdata[idx])\n",
        "    val_fold_y.append(labels['diagnose'][idx])\n",
        "\n",
        "  folds_info.append((np.array(train_fold_x), np.array(val_fold_x), np.array(train_fold_y), np.array(val_fold_y)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRQxw7CTi-tY",
        "outputId": "93b38cb1-0421-4583-bd63-299de35b87c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 0:\n",
            "  Train: index=[ 0  1  2  3  4  5  7  9 10 11 14 16 18 19 20 21 22 23 24 25 26 27 28 29\n",
            " 30 31 32 33 35 37 38 39]\n",
            "  Test:  index=[ 6  8 12 13 15 17 34 36]\n",
            "Fold 1:\n",
            "  Train: index=[ 1  2  3  4  5  6  7  8  9 10 12 13 14 15 16 17 18 19 20 21 23 24 26 27\n",
            " 28 29 30 31 33 34 36 39]\n",
            "  Test:  index=[ 0 11 22 25 32 35 37 38]\n",
            "Fold 2:\n",
            "  Train: index=[ 0  1  3  5  6  7  8  9 10 11 12 13 14 15 17 18 19 20 22 23 24 25 26 28\n",
            " 32 33 34 35 36 37 38 39]\n",
            "  Test:  index=[ 2  4 16 21 27 29 30 31]\n",
            "Fold 3:\n",
            "  Train: index=[ 0  1  2  3  4  6  8  9 10 11 12 13 15 16 17 21 22 24 25 26 27 28 29 30\n",
            " 31 32 34 35 36 37 38 39]\n",
            "  Test:  index=[ 5  7 14 18 19 20 23 33]\n",
            "Fold 4:\n",
            "  Train: index=[ 0  2  4  5  6  7  8 11 12 13 14 15 16 17 18 19 20 21 22 23 25 27 29 30\n",
            " 31 32 33 34 35 36 37 38]\n",
            "  Test:  index=[ 1  3  9 10 24 26 28 39]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(folds_info), len(folds_info[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6GGW8QfjOQK",
        "outputId": "397877c9-7466-42ce-c71d-05e50abc4daf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fold_0_X_train=folds_info[0][0]\n",
        "fold_0_y_train=folds_info[0][2]\n",
        "\n",
        "print(fold_0_X_train.shape)\n",
        "print(fold_0_y_train.shape)\n",
        "\n",
        "fold_1_X_train=folds_info[1][0]\n",
        "fold_1_y_train=folds_info[1][2]\n",
        "\n",
        "print(fold_1_X_train.shape)\n",
        "print(fold_1_y_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYHugzBcoP_E",
        "outputId": "ab4e3486-0912-48e0-91cf-2587000994df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32, 2, 64, 64, 64)\n",
            "(32,)\n",
            "(32, 2, 64, 64, 64)\n",
            "(32,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### select fold"
      ],
      "metadata": {
        "id": "NjnbRXLasGrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "fold_1"
      ],
      "metadata": {
        "id": "t3LvTmbXpwId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "i=0\n",
        "X_train, X_test, y_train, y_test=folds_info[i][0], folds_info[i][1], folds_info[i][2], folds_info[i][3]"
      ],
      "metadata": {
        "id": "Ye8jd-5VpuEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "fold_2"
      ],
      "metadata": {
        "id": "mZo0VNaGpx63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "i=1\n",
        "X_train, X_test, y_train, y_test=folds_info[i][0], folds_info[i][1], folds_info[i][2], folds_info[i][3]"
      ],
      "metadata": {
        "id": "PCfjbkajpt8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "fold_3"
      ],
      "metadata": {
        "id": "ywI6ZPxRpywz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "i=2\n",
        "X_train, X_test, y_train, y_test=folds_info[i][0], folds_info[i][1], folds_info[i][2], folds_info[i][3]"
      ],
      "metadata": {
        "id": "vcks2Yl-pt1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "fold_4"
      ],
      "metadata": {
        "id": "Wmw-8deGp1kA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "i=3\n",
        "X_train, X_test, y_train, y_test=folds_info[i][0], folds_info[i][1], folds_info[i][2], folds_info[i][3]"
      ],
      "metadata": {
        "id": "FhThQDIiptjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "fold_5"
      ],
      "metadata": {
        "id": "VEjktwrUp2P_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "i=4\n",
        "X_train, X_test, y_train, y_test=folds_info[i][0], folds_info[i][1], folds_info[i][2], folds_info[i][3]"
      ],
      "metadata": {
        "id": "2Le4Lx47ptbx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Create dataloaders and extract features"
      ],
      "metadata": {
        "id": "XwVjzmQtuyQS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mean, std = calculate_mean_std(X_train)\n",
        "print(\"Mean for each channel:\", mean)\n",
        "print(\"Standard deviation for each channel:\", std)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jx_gqOVsuy76",
        "outputId": "3e3b1b4d-094c-406b-9c67-23f159f8c501"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean for each channel: [12.50556362  3.101349  ]\n",
            "Standard deviation for each channel: [26.98893801 17.80137844]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define transformations for data augmentation\n",
        "# We only need and use validation transformation\n",
        "val_transforms = transforms.Compose([\n",
        "    ToTensor(),\n",
        "    Normalize3D(mean, std)\n",
        "])"
      ],
      "metadata": {
        "id": "xGViyusX5s6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create custom datasets with transformations\n",
        "train_dataset = CustomDataset(X_train, y_train, transform=val_transforms)\n",
        "val_dataset = CustomDataset(X_test, y_test, transform=val_transforms)  # Apply validation transformation\n",
        "\n",
        "print(f\"There are {len(train_dataset)} train images and {len(val_dataset)} val images\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2yZ4u_TYu1Fv",
        "outputId": "ebb0903d-cbb6-4d1c-b4c5-985a93bf9a8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 32 train images and 8 val images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('train dataset', np.unique(train_dataset.labels, return_counts=True))\n",
        "print('test dataset', np.unique(val_dataset.labels, return_counts=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8QuECKeu8bK",
        "outputId": "dd99a6ce-26c7-4f4f-837b-d3cfbf6f1a0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train dataset (array([1, 2, 3, 4]), array([8, 8, 8, 8]))\n",
            "test dataset (array([1, 2, 3, 4]), array([2, 2, 2, 2]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=1, num_workers=0, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=1, num_workers=0, shuffle=False)"
      ],
      "metadata": {
        "id": "_swhVC-uu_Pf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Feature extraction"
      ],
      "metadata": {
        "id": "9DZ787rL4orl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "# Define a function to extract features from the ResNet model\n",
        "def extract_features(model, dataloader):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    features = []\n",
        "    labels = []\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(dataloader):\n",
        "        # for inputs, targets in dataloader:\n",
        "            inputs, targets = batch['input'].to(device), batch['label'].to(device)\n",
        "            # Forward pass through the ResNet model\n",
        "            outputs = model(inputs)\n",
        "            temp = model.feature_extraction(inputs)\n",
        "            # Extract features before the fully connected layer\n",
        "            features.append(temp.numpy())\n",
        "            labels.append(targets.numpy())\n",
        "    return torch.tensor(features), torch.tensor(labels)\n",
        "\n",
        "# Extract features from the training and validation datasets\n",
        "train_features, train_labels = extract_features(loaded_model, train_loader)\n",
        "\n",
        "val_features, val_labels = extract_features(loaded_model, val_loader)\n",
        "\n",
        "# Flatten the features\n",
        "train_features = train_features.view(train_features.size(0), -1)\n",
        "train_labels = train_labels.view(train_labels.size(0))\n",
        "val_features = val_features.view(val_features.size(0), -1)\n",
        "val_labels = val_labels.view(val_labels.size(0))\n",
        "\n",
        "print(train_features.shape)\n",
        "print(train_labels.shape)\n",
        "print(val_features.shape)\n",
        "print(val_labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8SjoNOyvFii",
        "outputId": "3059ded7-2f3e-400a-abde-6adc1b64d715"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 512])\n",
            "torch.Size([32])\n",
            "torch.Size([8, 512])\n",
            "torch.Size([8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SVM classifier"
      ],
      "metadata": {
        "id": "XhVmBC_24n5k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score\n",
        "\n",
        "# Train an SVM classifier\n",
        "svm_classifier = svm.SVC(kernel='linear')\n",
        "svm_classifier.fit(train_features.numpy(), train_labels.numpy())\n",
        "\n",
        "# Predict using the SVM classifier\n",
        "predicted_labels = svm_classifier.predict(val_features.numpy())\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(val_labels.numpy(), predicted_labels)\n",
        "print(\"Validation Accuracy:\", accuracy)\n",
        "\n",
        "# Calculate confusion matrix\n",
        "conf_matrix = confusion_matrix(val_labels.numpy(), predicted_labels)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Calculate precision and recall\n",
        "precision = precision_score(val_labels.numpy(), predicted_labels, average=None)\n",
        "recall = recall_score(val_labels.numpy(), predicted_labels, average=None)\n",
        "\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_Ob5_AWvkyb",
        "outputId": "9f0ff75b-f263-40da-db15-9428aee1e925"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 1.0\n",
            "Confusion Matrix:\n",
            "[[2 0 0 0]\n",
            " [0 2 0 0]\n",
            " [0 0 2 0]\n",
            " [0 0 0 2]]\n",
            "Precision: [1. 1. 1. 1.]\n",
            "Recall: [1. 1. 1. 1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_preds5 = svm_classifier.predict(test_features)\n",
        "print(test_preds5.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-R3zPdOBwV0P",
        "outputId": "3eb50bd5-b230-4442-adf4-3b4c5a33dcbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(41,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_preds1+1)\n",
        "print(test_preds2+1)\n",
        "print(test_preds3+1)\n",
        "print(test_preds4+1)\n",
        "print(test_preds5+1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1U3W4a6lwYJB",
        "outputId": "5b869dfa-c3d4-4fc5-ad93-5a1632ca981e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3 2 3 1 1 1 2 2 3 2 3 1 2 4 2 4 1 1 1 3 1 3 3 2 3 3 4 2 2 2 1 2 2 1 1 3 3\n",
            " 1 3 3 4]\n",
            "[3 2 3 1 1 1 1 3 3 2 3 1 2 1 2 4 1 1 1 3 1 3 3 2 3 2 4 1 2 2 1 2 2 1 1 3 3\n",
            " 1 1 3 4]\n",
            "[3 2 3 1 1 1 1 3 3 2 3 1 2 1 2 4 1 1 1 3 1 3 3 2 3 2 4 1 2 2 1 2 2 1 1 3 3\n",
            " 1 1 3 2]\n",
            "[2 2 3 1 1 1 1 3 3 2 1 1 2 1 2 4 1 1 1 3 1 3 3 2 3 2 4 1 2 2 1 2 2 1 1 3 3\n",
            " 1 1 3 4]\n",
            "[3 2 3 1 1 1 1 3 3 2 3 1 2 1 2 4 1 1 1 3 1 3 3 2 3 3 2 1 2 2 1 2 2 1 1 3 3\n",
            " 1 1 3 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensemble test results prediction"
      ],
      "metadata": {
        "id": "Q1EjgrI8-1wo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pred1_np = np.array(test_preds1+1)\n",
        "pred2_np = np.array(test_preds2+1)\n",
        "pred3_np = np.array(test_preds3+1)\n",
        "pred4_np = np.array(test_preds4+1)\n",
        "pred5_np = np.array(test_preds5+1)\n",
        "\n",
        "\n",
        "preds = np.column_stack((pred1_np, pred2_np,pred3_np,pred4_np,pred5_np))\n",
        "print(preds.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3BTNvBoyRI_",
        "outputId": "b544dd3d-493d-4447-b907-7eeced2a0b63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(41, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import mode\n",
        "modes, counts = mode(preds, axis=1)\n",
        "most_frequent_values = modes.squeeze()\n",
        "\n",
        "print(most_frequent_values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZMMzyBIzbYb",
        "outputId": "18842261-7e67-47bf-c4d9-f921971fb72d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3 2 3 1 1 1 1 3 3 2 3 1 2 1 2 4 1 1 1 3 1 3 3 2 3 2 4 1 2 2 1 2 2 1 1 3 3\n",
            " 1 1 3 4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save the DataFrame to a CSV file, including patient number"
      ],
      "metadata": {
        "id": "z5y2iyTb-tjH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index_array = np.arange(1, most_frequent_values.shape[0]+1)\n",
        "combined_array = np.column_stack((index_array, most_frequent_values))\n",
        "\n",
        "df = pd.DataFrame(combined_array, columns=['patient_number', 'SVM_Kfold'])\n",
        "df.to_csv(\"patient_predictions.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "dVET8gwi0zKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final model\n",
        "Train our final model on all of training data.\n",
        "Predict labels for test data that we don't have their labels and save them on CSV file."
      ],
      "metadata": {
        "id": "PiIXgA9AHBUb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the test data"
      ],
      "metadata": {
        "id": "-Z4tJGaUHX9i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# path to train datasets, labels and VoI template files\n",
        "\n",
        "test_rCBF = Path(\"/content/drive/MyDrive/Assignment/test_images_rcbf.nii\")\n",
        "test_DAT = Path(\"/content/drive/MyDrive/Assignment/test_images_sbr.nii\")"
      ],
      "metadata": {
        "id": "1KBwyj_8HXyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load NIfTI and extract image data\n",
        "\n",
        "test_PET_rCBF = nib.load(test_rCBF)\n",
        "test_data_rCBF = test_PET_rCBF.get_fdata()\n",
        "\n",
        "test_PET_DAT = nib.load(test_DAT)\n",
        "test_data_DAT = test_PET_DAT.get_fdata()"
      ],
      "metadata": {
        "id": "ntWoS4h8HdhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset creation, feature engineering and feature normalization"
      ],
      "metadata": {
        "id": "zbElRYAyLggo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xdata = create_xdata(train_data_rCBF, train_data_DAT)\n",
        "print(xdata.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wOpFjJHo3F0",
        "outputId": "3080cb50-2125-4be8-f79f-b51b9b8bfb3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(40, 2, 64, 64, 64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xdata_test = create_xdata(test_data_rCBF, test_data_DAT)\n",
        "print(xdata_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oX9bsgCtH53p",
        "outputId": "10eef990-525f-432e-edf9-3836c60f1e7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(41, 2, 64, 64, 64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This time we are going to train our model on all of training data,\n",
        "# so we calcualte mean and std for all of them.\n",
        "\n",
        "mean, std = calculate_mean_std(xdata)\n",
        "print(\"Mean for each channel:\", mean.shape)\n",
        "print(\"Standard deviation for each channel:\", std.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B00TNDY0IAiU",
        "outputId": "9f6a5e85-d19f-4e6d-f770-78add0fe31a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean for each channel: (2,)\n",
            "Standard deviation for each channel: (2,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = CustomDataset(xdata, np.array(labels['diagnose']), transform=val_transforms)\n",
        "test_dataset = CustomDataset(xdata_test, np.zeros((xdata_test.shape[0])), transform=val_transforms)\n",
        "\n",
        "print(f\"There are {len(train_dataset)} train images and {len(test_dataset)} test images\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdO4gK5_pFem",
        "outputId": "cdd5d972-d64c-49ac-f1f3-7a38acbe3f0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 40 train images and 41 test images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('train dataset', np.unique(train_dataset.labels, return_counts=True))\n",
        "print('test_dataset', np.unique(test_dataset.labels, return_counts=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3cATijNlKxhG",
        "outputId": "f945020f-23de-4fbc-8ceb-e1e5ae810634"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train dataset (array([1, 2, 3, 4]), array([10, 10, 10, 10]))\n",
            "test_dataset (array([0.]), array([41]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=1, num_workers=0, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, num_workers=0, shuffle=False)"
      ],
      "metadata": {
        "id": "c__MIw55qTJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = CustomDataset(xdata_test, np.zeros((xdata_test.shape[0])), transform=val_transforms)\n",
        "\n",
        "print(f\"There are {len(test_dataset)} test images\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eyPZtG5QmZ52",
        "outputId": "2a1f1989-36e8-4345-b27f-d3b696d9ddeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 41 test images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loader = DataLoader(test_dataset, batch_size=1, num_workers=0, shuffle=False)\n",
        "print('test dataset', np.unique(test_dataset.labels, return_counts=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARrcTskgm2ks",
        "outputId": "0908b714-24aa-4b85-c57e-0756b8269c74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test dataset (array([0.]), array([41]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_features, test_labels = extract_features(loaded_model, test_loader)\n",
        "\n",
        "test_features = test_features.view(test_features.size(0), -1)\n",
        "test_labels = test_labels.view(test_labels.size(0))\n",
        "\n",
        "print(test_features.shape)\n",
        "print(test_labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HtD0kw1iwLGF",
        "outputId": "6ebbdab3-9e88-4a6b-a584-9368e0af94da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([41, 512])\n",
            "torch.Size([41])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_features, train_labels = extract_features(loaded_model, train_loader)\n",
        "test_features, test_labels = extract_features(loaded_model, test_loader)\n",
        "\n",
        "# Flatten the features\n",
        "train_features = train_features.view(train_features.size(0), -1)\n",
        "train_labels = train_labels.view(train_labels.size(0))\n",
        "test_features = test_features.view(test_features.size(0), -1)\n",
        "test_labels = test_labels.view(test_labels.size(0))\n",
        "\n",
        "print(train_features.shape)\n",
        "print(train_labels.shape)\n",
        "print(test_features.shape)\n",
        "print(test_labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c35ISrzvKzaC",
        "outputId": "769cf28b-db5f-415c-c288-f68fc12da1aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([40, 512])\n",
            "torch.Size([40])\n",
            "torch.Size([41, 512])\n",
            "torch.Size([41])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train SVM model on all the train data, predict on test and save to file"
      ],
      "metadata": {
        "id": "KxaLf8BgLpqU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train an SVM classifier\n",
        "svm_classifier2 = svm.SVC(kernel='linear')\n",
        "svm_classifier2.fit(train_features.numpy(), train_labels.numpy()+1)"
      ],
      "metadata": {
        "id": "NwoxNslvqzx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_preds2 = svm_classifier2.predict(test_features)\n",
        "print(test_preds2.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klCVRwAEq0wu",
        "outputId": "8a1c5966-ff3b-4c5e-e725-0b36aaf29137"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(41,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the DataFrame to a CSV file, including patient number\n",
        "\n",
        "index_array = np.arange(1, test_preds2.shape[0]+1)\n",
        "combined_array = np.column_stack((index_array, test_preds2))\n",
        "\n",
        "df = pd.DataFrame(combined_array, columns=['patient_number', 'SVM_predication'])\n",
        "df.to_csv(\"patient_predictions.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "6uU1M9ypq15I"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}